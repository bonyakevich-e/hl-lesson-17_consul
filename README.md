### OTUS High Load Lesson #17 | Subject: Consul cluster для service discovery и DNS
--------------------------------
### ЦЕЛЬ: Настроить Consul Service Discovery с healthcheck и consul-template для динамической генерации конфигурации nginx
--------------------------------
### ТРЕБОВАНИЯ:
- Реализовать consul cluster который выдает доменное имя для веб портала с прошлой ДЗ.
- Плавающий IP заменить на балансировку через DNS.
- В случае умирание одного из веб серверов IP должен убираться из DNS.

>[!NOTE]
>Я создал кластер из одной ноды с целью экономии ресурсов в yandex cloud и времени на развёртывание стенда. В продакшене нужно исользовать 3-5 нод в кластере для отказоустойчивости. Конфигурация трех-нодового кластера отличается от конфигурации однонодового несколькими строчками.
>Кроме того, дополнительно к настройке Consul Service Discovery я настроил consul-template, которые динамически пересоздаёт список бэкендов на load balancer'е в зависимости от доступности бэкенда.
-------------------------------
### ОЖИДАЕМЫЕ РЕЗУЛЬТАТЫ:
С учётом собственных правок:
- Виртуальная машины с сконфигурированным Consul server
- Две виртуальные машины backend, на которых запущен nginx. Также на этих машинах работает Consul client, который проверяет состояние nginx и регистрирует service на consul server'е
- Виртуальная машина с nginx, которая выступает в качестве балансировщика между бэкендами. Здесь же настроен consul-template, который динамически изменяет файл конфигурации nginx (добавляет/удаляет список адресов бэкендов) в зависимости от состояния сервисов на consul сервере
-------------------------------
### ВЫПОЛНЕНИЕ
Схема стенда: 

![otus highload scheme v4 consule](https://github.com/user-attachments/assets/7972c456-ddb7-4223-ba50-13b75dc2206e)

Стенд собирается с помощью команд:
```
$ terraform init
$ ansible-playbook -i hosts playbook.yml
```
В итоге имеем следующее.
Виртуальная машина с установленным Consul агентом, который выполняет роль сервера. Две виртуальные машины с установленным Consul агентом, который выполняет роль клиента:

![consul nodes](https://github.com/user-attachments/assets/7241445b-8ddb-4f98-88ed-79bee1a9313c)


Настроен service discovery, который мониторит nginx на бэкенд нодах:

![consule service](https://github.com/user-attachments/assets/7c918f62-63d6-4a58-ad5e-c5e0ec57dd01)

На frontend сервере, который выполняет роль балансировщика между бэкенд серверами, установлен consule-template. Он периодически обращается к consule server для проверки состояния сервиса backend. В зависимости от состояния сервиса consule-template обновляет список адресов бэкенд серверов в конфигурационном файле nginx. 
Например. Когда работает оба бэкенд сервера, конфигурационный файл балансировщика выглядит следующим образом:

![image](https://github.com/user-attachments/assets/146cb3b9-6d8d-48f6-b99c-13669537e57c)

Остановим nginx на одном из бэкенд серверов. В Consul UI видим что healtcheck отработал корректно и один и сервером стал недоступен:

![image](https://github.com/user-attachments/assets/5ee3e6d4-3dc1-46fa-8149-38fb68f313fe)

И видим адрес этого бэкенд сервера удалился из конфигурационного файла балансировщика:

![image](https://github.com/user-attachments/assets/86717d7f-ce0e-41ae-aafc-1c3785c3ca76)

Таким же образом при регистрации/удалении бэкенд нод, они автоматически буду добавляться/удаляться из балансировщика.

Также мы можем запрашивать с помощью DNS ip адреса нод, зарегестрированных на consul сервере. Например, запросим ip адрес сервера backend2:

![image](https://github.com/user-attachments/assets/bf55983f-90d9-4dbc-820f-c7b642a8c0c5)

